============================= test session starts =============================
platform win32 -- Python 3.13.9, pytest-9.0.2, pluggy-1.6.0 -- C:\Users\click\AppData\Local\Microsoft\WindowsApps\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\python.exe
cachedir: .pytest_cache
hypothesis profile 'default'
rootdir: C:\Users\click\OneDrive\Documents\mlm\market_liquidity_monitor
plugins: anyio-4.11.0, Faker-38.2.0, hypothesis-6.148.4, langsmith-0.5.1, logfire-4.16.0, asyncio-1.3.0, cov-7.0.0, mock-3.15.1
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 3 items

tests/test_database_simple.py::test_db_connection_fail_graceful FAILED
tests/test_database_simple.py::test_store_snapshot_mock DEBUG: session_factory type: <class 'unittest.mock.MagicMock'>
DEBUG: session type: <class 'unittest.mock.AsyncMock'>
DEBUG: session.begin type: <class 'unittest.mock.AsyncMock'>
DEBUG: begin_obj type: <class 'coroutine'>
FAILED
tests/test_database_simple.py::test_db_manager_singleton_import PASSED

================================== FAILURES ===================================
______________________ test_db_connection_fail_graceful _______________________

self = <market_liquidity_monitor.data_engine.database.DatabaseManager object at 0x0000023022FE4B90>

    async def connect(self):
        """Test connection and create tables."""
        try:
>           async with self.engine.begin() as conn:
                       ^^^^^^^^^^^^^^^^^^^

data_engine\database.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <contextlib._AsyncGeneratorContextManager object at 0x000002302810AA50>

    async def __aenter__(self):
        # do not keep args and kwds alive unnecessarily
        # they are only needed for recreation, which is not possible anymore
        del self.args, self.kwds, self.func
        try:
>           return await anext(self.gen)
                   ^^^^^^^^^^^^^^^^^^^^^

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\Lib\contextlib.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.ext.asyncio.engine.AsyncEngine object at 0x000002302819CE50>

    @contextlib.asynccontextmanager
    async def begin(self) -> AsyncIterator[AsyncConnection]:
        """Return a context manager which when entered will deliver an
        :class:`_asyncio.AsyncConnection` with an
        :class:`_asyncio.AsyncTransaction` established.
    
        E.g.::
    
            async with async_engine.begin() as conn:
                await conn.execute(
                    text("insert into table (x, y, z) values (1, 2, 3)")
                )
                await conn.execute(text("my_special_procedure(5)"))
    
        """
        conn = self.connect()
    
>       async with conn:
                   ^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\ext\asyncio\engine.py:1068: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.ext.asyncio.engine.AsyncConnection object at 0x000002302811E3A0>

    async def __aenter__(self) -> _T_co:
>       return await self.start(is_ctxmanager=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\ext\asyncio\base.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.ext.asyncio.engine.AsyncConnection object at 0x000002302811E3A0>
is_ctxmanager = True

    async def start(
        self, is_ctxmanager: bool = False  # noqa: U100
    ) -> AsyncConnection:
        """Start this :class:`_asyncio.AsyncConnection` object's context
        outside of using a Python ``with:`` block.
    
        """
        if self.sync_connection:
            raise exc.InvalidRequestError("connection is already started")
        self.sync_connection = self._assign_proxied(
>           await greenlet_spawn(self.sync_engine.connect)
        )

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\ext\asyncio\engine.py:275: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fn = <bound method Engine.connect of Engine(postgresql+asyncpg://invalid:***@localhost:5432/invalid)>
_require_await = False, args = (), kwargs = {}
context = <_AsyncIoGreenlet object at 0x000002302819D840 (otid=0x0000023026E67630) dead>
switch_occurred = True
result = <coroutine object connect at 0x00000230281A04F0>

    async def greenlet_spawn(
        fn: Callable[..., _T],
        *args: Any,
        _require_await: bool = False,
        **kwargs: Any,
    ) -> _T:
        """Runs a sync function ``fn`` in a new greenlet.
    
        The sync function can then use :func:`await_only` to wait for async
        functions.
    
        :param fn: The sync callable to call.
        :param \\*args: Positional arguments to pass to the ``fn`` callable.
        :param \\*\\*kwargs: Keyword arguments to pass to the ``fn`` callable.
        """
    
        result: Any
        context = _AsyncIoGreenlet(fn, getcurrent())
        # runs the function synchronously in gl greenlet. If the execution
        # is interrupted by await_only, context is not dead and result is a
        # coroutine to wait. If the context is dead the function has
        # returned, and its result can be returned.
        switch_occurred = False
        result = context.switch(*args, **kwargs)
        while not context.dead:
            switch_occurred = True
            try:
                # wait for a coroutine from await_only and then return its
                # result back to it.
                value = await result
            except BaseException:
                # this allows an exception to be raised within
                # the moderated greenlet so that it can continue
                # its expected flow.
>               result = context.throw(*sys.exc_info())
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\util\_concurrency_py3k.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Engine(postgresql+asyncpg://invalid:***@localhost:5432/invalid)

    def connect(self) -> Connection:
        """Return a new :class:`_engine.Connection` object.
    
        The :class:`_engine.Connection` acts as a Python context manager, so
        the typical use of this method looks like::
    
            with engine.connect() as connection:
                connection.execute(text("insert into table values ('foo')"))
                connection.commit()
    
        Where above, after the block is completed, the connection is "closed"
        and its underlying DBAPI resources are returned to the connection pool.
        This also has the effect of rolling back any transaction that
        was explicitly begun or was begun via autobegin, and will
        emit the :meth:`_events.ConnectionEvents.rollback` event if one was
        started and is still in progress.
    
        .. seealso::
    
            :meth:`_engine.Engine.begin`
    
        """
    
>       return self._connection_cls(self)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\engine\base.py:3285: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.engine.base.Connection object at 0x000002302810ACF0>
engine = Engine(postgresql+asyncpg://invalid:***@localhost:5432/invalid)
connection = None, _has_events = None, _allow_revalidate = True
_allow_autobegin = True

    def __init__(
        self,
        engine: Engine,
        connection: Optional[PoolProxiedConnection] = None,
        _has_events: Optional[bool] = None,
        _allow_revalidate: bool = True,
        _allow_autobegin: bool = True,
    ):
        """Construct a new Connection."""
        self.engine = engine
        self.dialect = dialect = engine.dialect
    
        if connection is None:
            try:
>               self._dbapi_connection = engine.raw_connection()
                                         ^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\engine\base.py:143: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Engine(postgresql+asyncpg://invalid:***@localhost:5432/invalid)

    def raw_connection(self) -> PoolProxiedConnection:
        """Return a "raw" DBAPI connection from the connection pool.
    
        The returned object is a proxied version of the DBAPI
        connection object used by the underlying driver in use.
        The object will have all the same behavior as the real DBAPI
        connection, except that its ``close()`` method will result in the
        connection being returned to the pool, rather than being closed
        for real.
    
        This method provides direct DBAPI connection access for
        special situations when the API provided by
        :class:`_engine.Connection`
        is not needed.   When a :class:`_engine.Connection` object is already
        present, the DBAPI connection is available using
        the :attr:`_engine.Connection.connection` accessor.
    
        .. seealso::
    
            :ref:`dbapi_connections`
    
        """
>       return self.pool.connect()
               ^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\engine\base.py:3309: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.pool.impl.AsyncAdaptedQueuePool object at 0x00000230280AD6D0>

    def connect(self) -> PoolProxiedConnection:
        """Return a DBAPI connection from the pool.
    
        The connection is instrumented such that when its
        ``close()`` method is called, the connection will be returned to
        the pool.
    
        """
>       return _ConnectionFairy._checkout(self)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\base.py:447: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'sqlalchemy.pool.base._ConnectionFairy'>
pool = <sqlalchemy.pool.impl.AsyncAdaptedQueuePool object at 0x00000230280AD6D0>
threadconns = None, fairy = None

    @classmethod
    def _checkout(
        cls,
        pool: Pool,
        threadconns: Optional[threading.local] = None,
        fairy: Optional[_ConnectionFairy] = None,
    ) -> _ConnectionFairy:
        if not fairy:
>           fairy = _ConnectionRecord.checkout(pool)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\base.py:1264: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'sqlalchemy.pool.base._ConnectionRecord'>
pool = <sqlalchemy.pool.impl.AsyncAdaptedQueuePool object at 0x00000230280AD6D0>

    @classmethod
    def checkout(cls, pool: Pool) -> _ConnectionFairy:
        if TYPE_CHECKING:
            rec = cast(_ConnectionRecord, pool._do_get())
        else:
>           rec = pool._do_get()
                  ^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\base.py:711: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.pool.impl.AsyncAdaptedQueuePool object at 0x00000230280AD6D0>

    def _do_get(self) -> ConnectionPoolEntry:
        use_overflow = self._max_overflow > -1
    
        wait = use_overflow and self._overflow >= self._max_overflow
        try:
            return self._pool.get(wait, self._timeout)
        except sqla_queue.Empty:
            # don't do things inside of "except Empty", because when we say
            # we timed out or can't connect and raise, Python 3 tells
            # people the real error is queue.Empty which it isn't.
            pass
        if use_overflow and self._overflow >= self._max_overflow:
            if not wait:
                return self._do_get()
            else:
                raise exc.TimeoutError(
                    "QueuePool limit of size %d overflow %d reached, "
                    "connection timed out, timeout %0.2f"
                    % (self.size(), self.overflow(), self._timeout),
                    code="3o7r",
                )
    
        if self._inc_overflow():
            try:
                return self._create_connection()
            except:
>               with util.safe_reraise():
                     ^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\impl.py:177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.util.langhelpers.safe_reraise object at 0x0000023026E49450>
type_ = None, value = None, traceback = None

    def __exit__(
        self,
        type_: Optional[Type[BaseException]],
        value: Optional[BaseException],
        traceback: Optional[types.TracebackType],
    ) -> NoReturn:
        assert self._exc_info is not None
        # see #2703 for notes
        if type_ is None:
            exc_type, exc_value, exc_tb = self._exc_info
            assert exc_value is not None
            self._exc_info = None  # remove potential circular references
>           raise exc_value.with_traceback(exc_tb)

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\util\langhelpers.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.pool.impl.AsyncAdaptedQueuePool object at 0x00000230280AD6D0>

    def _do_get(self) -> ConnectionPoolEntry:
        use_overflow = self._max_overflow > -1
    
        wait = use_overflow and self._overflow >= self._max_overflow
        try:
            return self._pool.get(wait, self._timeout)
        except sqla_queue.Empty:
            # don't do things inside of "except Empty", because when we say
            # we timed out or can't connect and raise, Python 3 tells
            # people the real error is queue.Empty which it isn't.
            pass
        if use_overflow and self._overflow >= self._max_overflow:
            if not wait:
                return self._do_get()
            else:
                raise exc.TimeoutError(
                    "QueuePool limit of size %d overflow %d reached, "
                    "connection timed out, timeout %0.2f"
                    % (self.size(), self.overflow(), self._timeout),
                    code="3o7r",
                )
    
        if self._inc_overflow():
            try:
>               return self._create_connection()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\impl.py:175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.pool.impl.AsyncAdaptedQueuePool object at 0x00000230280AD6D0>

    def _create_connection(self) -> ConnectionPoolEntry:
        """Called by subclasses to create a new ConnectionRecord."""
    
>       return _ConnectionRecord(self)
               ^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\base.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.pool.base._ConnectionRecord object at 0x00000230281015B0>
pool = <sqlalchemy.pool.impl.AsyncAdaptedQueuePool object at 0x00000230280AD6D0>
connect = True

    def __init__(self, pool: Pool, connect: bool = True):
        self.fresh = False
        self.fairy_ref = None
        self.starttime = 0
        self.dbapi_connection = None
    
        self.__pool = pool
        if connect:
>           self.__connect()

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.pool.base._ConnectionRecord object at 0x00000230281015B0>

    def __connect(self) -> None:
        pool = self.__pool
    
        # ensure any existing connection is removed, so that if
        # creator fails, this attribute stays None
        self.dbapi_connection = None
        try:
            self.starttime = time.time()
            self.dbapi_connection = connection = pool._invoke_creator(self)
            pool.logger.debug("Created new connection %r", connection)
            self.fresh = True
        except BaseException as e:
>           with util.safe_reraise():
                 ^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\base.py:899: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.util.langhelpers.safe_reraise object at 0x0000023027DBFE50>
type_ = None, value = None, traceback = None

    def __exit__(
        self,
        type_: Optional[Type[BaseException]],
        value: Optional[BaseException],
        traceback: Optional[types.TracebackType],
    ) -> NoReturn:
        assert self._exc_info is not None
        # see #2703 for notes
        if type_ is None:
            exc_type, exc_value, exc_tb = self._exc_info
            assert exc_value is not None
            self._exc_info = None  # remove potential circular references
>           raise exc_value.with_traceback(exc_tb)

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\util\langhelpers.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.pool.base._ConnectionRecord object at 0x00000230281015B0>

    def __connect(self) -> None:
        pool = self.__pool
    
        # ensure any existing connection is removed, so that if
        # creator fails, this attribute stays None
        self.dbapi_connection = None
        try:
            self.starttime = time.time()
>           self.dbapi_connection = connection = pool._invoke_creator(self)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\pool\base.py:895: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

connection_record = <sqlalchemy.pool.base._ConnectionRecord object at 0x00000230281015B0>

    def connect(
        connection_record: Optional[ConnectionPoolEntry] = None,
    ) -> DBAPIConnection:
        if dialect._has_events:
            for fn in dialect.dispatch.do_connect:
                connection = cast(
                    DBAPIConnection,
                    fn(dialect, connection_record, cargs, cparams),
                )
                if connection is not None:
                    return connection
    
>       return dialect.connect(*cargs, **cparams)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\engine\create.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.PGDialect_asyncpg object at 0x00000230280AD310>
cargs = ()
cparams = {'database': 'invalid', 'host': 'localhost', 'password': 'invalid', 'port': 5432, ...}

    def connect(self, *cargs: Any, **cparams: Any) -> DBAPIConnection:
        # inherits the docstring from interfaces.Dialect.connect
>       return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\engine\default.py:630: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_dbapi object at 0x00000230280AD1D0>
arg = ()
kw = {'database': 'invalid', 'host': 'localhost', 'password': 'invalid', 'port': 5432, ...}
async_fallback = False, creator_fn = <function connect at 0x000002302808AF20>
prepared_statement_cache_size = 100, prepared_statement_name_func = None

    def connect(self, *arg, **kw):
        async_fallback = kw.pop("async_fallback", False)
        creator_fn = kw.pop("async_creator_fn", self.asyncpg.connect)
        prepared_statement_cache_size = kw.pop(
            "prepared_statement_cache_size", 100
        )
        prepared_statement_name_func = kw.pop(
            "prepared_statement_name_func", None
        )
    
        if util.asbool(async_fallback):
            return AsyncAdaptFallback_asyncpg_connection(
                self,
                await_fallback(creator_fn(*arg, **kw)),
                prepared_statement_cache_size=prepared_statement_cache_size,
                prepared_statement_name_func=prepared_statement_name_func,
            )
        else:
            return AsyncAdapt_asyncpg_connection(
                self,
>               await_only(creator_fn(*arg, **kw)),
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                prepared_statement_cache_size=prepared_statement_cache_size,
                prepared_statement_name_func=prepared_statement_name_func,
            )

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:955: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

awaitable = <coroutine object connect at 0x00000230281A04F0>

    def await_only(awaitable: Awaitable[_T]) -> _T:
        """Awaits an async function in a sync method.
    
        The sync method must be inside a :func:`greenlet_spawn` context.
        :func:`await_only` calls cannot be nested.
    
        :param awaitable: The coroutine to call.
    
        """
        # this is called in the context greenlet while running fn
        current = getcurrent()
        if not getattr(current, "__sqlalchemy_greenlet_provider__", False):
            _safe_cancel_awaitable(awaitable)
    
            raise exc.MissingGreenlet(
                "greenlet_spawn has not been called; can't call await_only() "
                "here. Was IO attempted in an unexpected place?"
            )
    
        # returns the control to the driver greenlet passing it
        # a coroutine to run. Once the awaitable is done, the driver greenlet
        # switches back to this greenlet with the result of awaitable that is
        # then returned to the caller (or raised as error)
>       return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\util\_concurrency_py3k.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fn = <bound method Engine.connect of Engine(postgresql+asyncpg://invalid:***@localhost:5432/invalid)>
_require_await = False, args = (), kwargs = {}
context = <_AsyncIoGreenlet object at 0x000002302819D840 (otid=0x0000023026E67630) dead>
switch_occurred = True
result = <coroutine object connect at 0x00000230281A04F0>

    async def greenlet_spawn(
        fn: Callable[..., _T],
        *args: Any,
        _require_await: bool = False,
        **kwargs: Any,
    ) -> _T:
        """Runs a sync function ``fn`` in a new greenlet.
    
        The sync function can then use :func:`await_only` to wait for async
        functions.
    
        :param fn: The sync callable to call.
        :param \\*args: Positional arguments to pass to the ``fn`` callable.
        :param \\*\\*kwargs: Keyword arguments to pass to the ``fn`` callable.
        """
    
        result: Any
        context = _AsyncIoGreenlet(fn, getcurrent())
        # runs the function synchronously in gl greenlet. If the execution
        # is interrupted by await_only, context is not dead and result is a
        # coroutine to wait. If the context is dead the function has
        # returned, and its result can be returned.
        switch_occurred = False
        result = context.switch(*args, **kwargs)
        while not context.dead:
            switch_occurred = True
            try:
                # wait for a coroutine from await_only and then return its
                # result back to it.
>               value = await result
                        ^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\sqlalchemy\util\_concurrency_py3k.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

dsn = None

    async def connect(dsn=None, *,
                      host=None, port=None,
                      user=None, password=None, passfile=None,
                      service=None,
                      servicefile=None,
                      database=None,
                      loop=None,
                      timeout=60,
                      statement_cache_size=100,
                      max_cached_statement_lifetime=300,
                      max_cacheable_statement_size=1024 * 15,
                      command_timeout=None,
                      ssl=None,
                      direct_tls=None,
                      connection_class=Connection,
                      record_class=protocol.Record,
                      server_settings=None,
                      target_session_attrs=None,
                      krbsrvname=None,
                      gsslib=None):
        r"""A coroutine to establish a connection to a PostgreSQL server.
    
        The connection parameters may be specified either as a connection
        URI in *dsn*, or as specific keyword arguments, or both.
        If both *dsn* and keyword arguments are specified, the latter
        override the corresponding values parsed from the connection URI.
        The default values for the majority of arguments can be specified
        using `environment variables <postgres envvars_>`_.
    
        Returns a new :class:`~asyncpg.connection.Connection` object.
    
        :param dsn:
            Connection arguments specified using as a single string in the
            `libpq connection URI format`_:
            ``postgres://user:password@host:port/database?option=value``.
            The following options are recognized by asyncpg: ``host``,
            ``port``, ``user``, ``database`` (or ``dbname``), ``password``,
            ``passfile``, ``sslmode``, ``sslcert``, ``sslkey``, ``sslrootcert``,
            and ``sslcrl``.  Unlike libpq, asyncpg will treat unrecognized
            options as `server settings`_ to be used for the connection.
    
            .. note::
    
               The URI must be *valid*, which means that all components must
               be properly quoted with :py:func:`urllib.parse.quote_plus`, and
               any literal IPv6 addresses must be enclosed in square brackets.
               For example:
    
               .. code-block:: text
    
                  postgres://dbuser@[fe80::1ff:fe23:4567:890a%25eth0]/dbname
    
        :param host:
            Database host address as one of the following:
    
            - an IP address or a domain name;
            - an absolute path to the directory containing the database
              server Unix-domain socket (not supported on Windows);
            - a sequence of any of the above, in which case the addresses
              will be tried in order, and the first successful connection
              will be returned.
    
            If not specified, asyncpg will try the following, in order:
    
            - host address(es) parsed from the *dsn* argument,
            - the value of the ``PGHOST`` environment variable,
            - on Unix, common directories used for PostgreSQL Unix-domain
              sockets: ``"/run/postgresql"``, ``"/var/run/postgresl"``,
              ``"/var/pgsql_socket"``, ``"/private/tmp"``, and ``"/tmp"``,
            - ``"localhost"``.
    
        :param port:
            Port number to connect to at the server host
            (or Unix-domain socket file extension).  If multiple host
            addresses were specified, this parameter may specify a
            sequence of port numbers of the same length as the host sequence,
            or it may specify a single port number to be used for all host
            addresses.
    
            If not specified, the value parsed from the *dsn* argument is used,
            or the value of the ``PGPORT`` environment variable, or ``5432`` if
            neither is specified.
    
        :param user:
            The name of the database role used for authentication.
    
            If not specified, the value parsed from the *dsn* argument is used,
            or the value of the ``PGUSER`` environment variable, or the
            operating system name of the user running the application.
    
        :param database:
            The name of the database to connect to.
    
            If not specified, the value parsed from the *dsn* argument is used,
            or the value of the ``PGDATABASE`` environment variable, or the
            computed value of the *user* argument.
    
        :param password:
            Password to be used for authentication, if the server requires
            one.  If not specified, the value parsed from the *dsn* argument
            is used, or the value of the ``PGPASSWORD`` environment variable.
            Note that the use of the environment variable is discouraged as
            other users and applications may be able to read it without needing
            specific privileges.  It is recommended to use *passfile* instead.
    
            Password may be either a string, or a callable that returns a string.
            If a callable is provided, it will be called each time a new connection
            is established.
    
        :param passfile:
            The name of the file used to store passwords
            (defaults to ``~/.pgpass``, or ``%APPDATA%\postgresql\pgpass.conf``
            on Windows).
    
        :param service:
            The name of the postgres connection service stored in the postgres
            connection service file.
    
        :param servicefile:
            The location of the connnection service file used to store
            connection parameters.
    
        :param loop:
            An asyncio event loop instance.  If ``None``, the default
            event loop will be used.
    
        :param float timeout:
            Connection timeout in seconds.
    
        :param int statement_cache_size:
            The size of prepared statement LRU cache.  Pass ``0`` to
            disable the cache.
    
        :param int max_cached_statement_lifetime:
            The maximum time in seconds a prepared statement will stay
            in the cache.  Pass ``0`` to allow statements be cached
            indefinitely.
    
        :param int max_cacheable_statement_size:
            The maximum size of a statement that can be cached (15KiB by
            default).  Pass ``0`` to allow all statements to be cached
            regardless of their size.
    
        :param float command_timeout:
            The default timeout for operations on this connection
            (the default is ``None``: no timeout).
    
        :param ssl:
            Pass ``True`` or an `ssl.SSLContext <SSLContext_>`_ instance to
            require an SSL connection.  If ``True``, a default SSL context
            returned by `ssl.create_default_context() <create_default_context_>`_
            will be used.  The value can also be one of the following strings:
    
            - ``'disable'`` - SSL is disabled (equivalent to ``False``)
            - ``'prefer'`` - try SSL first, fallback to non-SSL connection
              if SSL connection fails
            - ``'allow'`` - try without SSL first, then retry with SSL if the first
              attempt fails.
            - ``'require'`` - only try an SSL connection.  Certificate
              verification errors are ignored
            - ``'verify-ca'`` - only try an SSL connection, and verify
              that the server certificate is issued by a trusted certificate
              authority (CA)
            - ``'verify-full'`` - only try an SSL connection, verify
              that the server certificate is issued by a trusted CA and
              that the requested server host name matches that in the
              certificate.
    
            The default is ``'prefer'``: try an SSL connection and fallback to
            non-SSL connection if that fails.
    
            .. note::
    
               *ssl* is ignored for Unix domain socket communication.
    
            Example of programmatic SSL context configuration that is equivalent
            to ``sslmode=verify-full&sslcert=..&sslkey=..&sslrootcert=..``:
    
            .. code-block:: pycon
    
                >>> import asyncpg
                >>> import asyncio
                >>> import ssl
                >>> async def main():
                ...     # Load CA bundle for server certificate verification,
                ...     # equivalent to sslrootcert= in DSN.
                ...     sslctx = ssl.create_default_context(
                ...         ssl.Purpose.SERVER_AUTH,
                ...         cafile="path/to/ca_bundle.pem")
                ...     # If True, equivalent to sslmode=verify-full, if False:
                ...     # sslmode=verify-ca.
                ...     sslctx.check_hostname = True
                ...     # Load client certificate and private key for client
                ...     # authentication, equivalent to sslcert= and sslkey= in
                ...     # DSN.
                ...     sslctx.load_cert_chain(
                ...         "path/to/client.cert",
                ...         keyfile="path/to/client.key",
                ...     )
                ...     con = await asyncpg.connect(user='postgres', ssl=sslctx)
                ...     await con.close()
                >>> asyncio.run(main())
    
            Example of programmatic SSL context configuration that is equivalent
            to ``sslmode=require`` (no server certificate or host verification):
    
            .. code-block:: pycon
    
                >>> import asyncpg
                >>> import asyncio
                >>> import ssl
                >>> async def main():
                ...     sslctx = ssl.create_default_context(
                ...         ssl.Purpose.SERVER_AUTH)
                ...     sslctx.check_hostname = False
                ...     sslctx.verify_mode = ssl.CERT_NONE
                ...     con = await asyncpg.connect(user='postgres', ssl=sslctx)
                ...     await con.close()
                >>> asyncio.run(main())
    
        :param bool direct_tls:
            Pass ``True`` to skip PostgreSQL STARTTLS mode and perform a direct
            SSL connection. Must be used alongside ``ssl`` param.
    
        :param dict server_settings:
            An optional dict of server runtime parameters.  Refer to
            PostgreSQL documentation for
            a `list of supported options <server settings_>`_.
    
        :param type connection_class:
            Class of the returned connection object.  Must be a subclass of
            :class:`~asyncpg.connection.Connection`.
    
        :param type record_class:
            If specified, the class to use for records returned by queries on
            this connection object.  Must be a subclass of
            :class:`~asyncpg.Record`.
    
        :param SessionAttribute target_session_attrs:
            If specified, check that the host has the correct attribute.
            Can be one of:
    
            - ``"any"`` - the first successfully connected host
            - ``"primary"`` - the host must NOT be in hot standby mode
            - ``"standby"`` - the host must be in hot standby mode
            - ``"read-write"`` - the host must allow writes
            - ``"read-only"`` - the host most NOT allow writes
            - ``"prefer-standby"`` - first try to find a standby host, but if
              none of the listed hosts is a standby server,
              return any of them.
    
            If not specified, the value parsed from the *dsn* argument is used,
            or the value of the ``PGTARGETSESSIONATTRS`` environment variable,
            or ``"any"`` if neither is specified.
    
        :param str krbsrvname:
            Kerberos service name to use when authenticating with GSSAPI. This
            must match the server configuration. Defaults to 'postgres'.
    
        :param str gsslib:
            GSS library to use for GSSAPI/SSPI authentication. Can be 'gssapi'
            or 'sspi'. Defaults to 'sspi' on Windows and 'gssapi' otherwise.
    
        :return: A :class:`~asyncpg.connection.Connection` instance.
    
        Example:
    
        .. code-block:: pycon
    
            >>> import asyncpg
            >>> import asyncio
            >>> async def run():
            ...     con = await asyncpg.connect(user='postgres')
            ...     types = await con.fetch('SELECT * FROM pg_type')
            ...     print(types)
            ...
            >>> asyncio.run(run())
            [<Record typname='bool' typnamespace=11 ...
    
        .. versionadded:: 0.10.0
           Added ``max_cached_statement_use_count`` parameter.
    
        .. versionchanged:: 0.11.0
           Removed ability to pass arbitrary keyword arguments to set
           server settings.  Added a dedicated parameter ``server_settings``
           for that.
    
        .. versionadded:: 0.11.0
           Added ``connection_class`` parameter.
    
        .. versionadded:: 0.16.0
           Added ``passfile`` parameter
           (and support for password files in general).
    
        .. versionadded:: 0.18.0
           Added ability to specify multiple hosts in the *dsn*
           and *host* arguments.
    
        .. versionchanged:: 0.21.0
           The *password* argument now accepts a callable or an async function.
    
        .. versionchanged:: 0.22.0
           Added the *record_class* parameter.
    
        .. versionchanged:: 0.22.0
           The *ssl* argument now defaults to ``'prefer'``.
    
        .. versionchanged:: 0.24.0
           The ``sslcert``, ``sslkey``, ``sslrootcert``, and ``sslcrl`` options
           are supported in the *dsn* argument.
    
        .. versionchanged:: 0.25.0
           The ``sslpassword``, ``ssl_min_protocol_version``,
           and ``ssl_max_protocol_version`` options are supported in the *dsn*
           argument.
    
        .. versionchanged:: 0.25.0
           Default system root CA certificates won't be loaded when specifying a
           particular sslmode, following the same behavior in libpq.
    
        .. versionchanged:: 0.25.0
           The ``sslcert``, ``sslkey``, ``sslrootcert``, and ``sslcrl`` options
           in the *dsn* argument now have consistent default values of files under
           ``~/.postgresql/`` as libpq.
    
        .. versionchanged:: 0.26.0
           Added the *direct_tls* parameter.
    
        .. versionchanged:: 0.28.0
           Added the *target_session_attrs* parameter.
    
        .. versionchanged:: 0.30.0
           Added the *krbsrvname* and *gsslib* parameters.
    
        .. versionchanged:: 0.31.0
           Added the *servicefile* and *service* parameters.
    
        .. _SSLContext: https://docs.python.org/3/library/ssl.html#ssl.SSLContext
        .. _create_default_context:
            https://docs.python.org/3/library/ssl.html#ssl.create_default_context
        .. _server settings:
            https://www.postgresql.org/docs/current/static/runtime-config.html
        .. _postgres envvars:
            https://www.postgresql.org/docs/current/static/libpq-envars.html
        .. _libpq connection URI format:
            https://www.postgresql.org/docs/current/static/
            libpq-connect.html#LIBPQ-CONNSTRING
        """
        if not issubclass(connection_class, Connection):
            raise exceptions.InterfaceError(
                'connection_class is expected to be a subclass of '
                'asyncpg.Connection, got {!r}'.format(connection_class))
    
        if record_class is not protocol.Record:
            _check_record_class(record_class)
    
        if loop is None:
            loop = asyncio.get_event_loop()
    
        async with compat.timeout(timeout):
>           return await connect_utils._connect(
                loop=loop,
                connection_class=connection_class,
                record_class=record_class,
                dsn=dsn,
                host=host,
                port=port,
                user=user,
                password=password,
                passfile=passfile,
                service=service,
                servicefile=servicefile,
                ssl=ssl,
                direct_tls=direct_tls,
                database=database,
                server_settings=server_settings,
                command_timeout=command_timeout,
                statement_cache_size=statement_cache_size,
                max_cached_statement_lifetime=max_cached_statement_lifetime,
                max_cacheable_statement_size=max_cacheable_statement_size,
                target_session_attrs=target_session_attrs,
                krbsrvname=krbsrvname,
                gsslib=gsslib,
            )

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\asyncpg\connection.py:2443: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

loop = <ProactorEventLoop running=False closed=False debug=False>
connection_class = <class 'asyncpg.connection.Connection'>
record_class = <class 'asyncpg.protocol.record.Record'>
kwargs = {'command_timeout': None, 'database': 'invalid', 'direct_tls': None, 'dsn': None, ...}
addrs = [('localhost', 5432)]
params = ConnectionParameters(user='invalid', password='invalid', database='invalid', ssl=<ssl.SSLContext object at 0x000002302... 'postgres'>, server_settings=None, target_session_attrs=<SessionAttribute.any: 'any'>, krbsrvname=None, gsslib='sspi')
config = ConnectionConfiguration(command_timeout=None, statement_cache_size=100, max_cached_statement_lifetime=300, max_cacheable_statement_size=15360)
target_attr = <SessionAttribute.any: 'any'>

    async def _connect(*, loop, connection_class, record_class, **kwargs):
        if loop is None:
            loop = asyncio.get_event_loop()
    
        addrs, params, config = _parse_connect_arguments(**kwargs)
        target_attr = params.target_session_attrs
    
        candidates = []
        chosen_connection = None
        last_error = None
        try:
            for addr in addrs:
                try:
                    conn = await _connect_addr(
                        addr=addr,
                        loop=loop,
                        params=params,
                        config=config,
                        connection_class=connection_class,
                        record_class=record_class,
                    )
                    candidates.append(conn)
                    if await _can_use_connection(conn, target_attr):
                        chosen_connection = conn
                        break
                except OSError as ex:
                    last_error = ex
            else:
                if target_attr == SessionAttribute.prefer_standby and candidates:
                    chosen_connection = random.choice(candidates)
        finally:
    
            async def _close_candidates(conns, chosen):
                await asyncio.gather(
                    *(c.close() for c in conns if c is not chosen),
                    return_exceptions=True
                )
            if candidates:
                asyncio.create_task(
                    _close_candidates(candidates, chosen_connection))
    
        if chosen_connection:
            return chosen_connection
    
>       raise last_error or exceptions.TargetServerAttributeNotMatched(
            'None of the hosts match the target attribute requirement '
            '{!r}'.format(target_attr)
        )

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\asyncpg\connect_utils.py:1249: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

loop = <ProactorEventLoop running=False closed=False debug=False>
connection_class = <class 'asyncpg.connection.Connection'>
record_class = <class 'asyncpg.protocol.record.Record'>
kwargs = {'command_timeout': None, 'database': 'invalid', 'direct_tls': None, 'dsn': None, ...}
addrs = [('localhost', 5432)]
params = ConnectionParameters(user='invalid', password='invalid', database='invalid', ssl=<ssl.SSLContext object at 0x000002302... 'postgres'>, server_settings=None, target_session_attrs=<SessionAttribute.any: 'any'>, krbsrvname=None, gsslib='sspi')
config = ConnectionConfiguration(command_timeout=None, statement_cache_size=100, max_cached_statement_lifetime=300, max_cacheable_statement_size=15360)
target_attr = <SessionAttribute.any: 'any'>

    async def _connect(*, loop, connection_class, record_class, **kwargs):
        if loop is None:
            loop = asyncio.get_event_loop()
    
        addrs, params, config = _parse_connect_arguments(**kwargs)
        target_attr = params.target_session_attrs
    
        candidates = []
        chosen_connection = None
        last_error = None
        try:
            for addr in addrs:
                try:
>                   conn = await _connect_addr(
                        addr=addr,
                        loop=loop,
                        params=params,
                        config=config,
                        connection_class=connection_class,
                        record_class=record_class,
                    )

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\asyncpg\connect_utils.py:1218: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    async def _connect_addr(
        *,
        addr,
        loop,
        params,
        config,
        connection_class,
        record_class
    ):
        assert loop is not None
    
        params_input = params
        if callable(params.password):
            password = params.password()
            if inspect.isawaitable(password):
                password = await password
    
            params = params._replace(password=password)
        args = (addr, loop, config, connection_class, record_class, params_input)
    
        # prepare the params (which attempt has ssl) for the 2 attempts
        if params.sslmode == SSLMode.allow:
            params_retry = params
            params = params._replace(ssl=None)
        elif params.sslmode == SSLMode.prefer:
            params_retry = params._replace(ssl=None)
        else:
            # skip retry if we don't have to
            return await __connect_addr(params, False, *args)
    
        # first attempt
        try:
>           return await __connect_addr(params, True, *args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\asyncpg\connect_utils.py:1054: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

params = ConnectionParameters(user='invalid', password='invalid', database='invalid', ssl=<ssl.SSLContext object at 0x000002302... 'postgres'>, server_settings=None, target_session_attrs=<SessionAttribute.any: 'any'>, krbsrvname=None, gsslib='sspi')
retry = True, addr = ('localhost', 5432)
loop = <ProactorEventLoop running=False closed=False debug=False>
config = ConnectionConfiguration(command_timeout=None, statement_cache_size=100, max_cached_statement_lifetime=300, max_cacheable_statement_size=15360)
connection_class = <class 'asyncpg.connection.Connection'>
record_class = <class 'asyncpg.protocol.record.Record'>
params_input = ConnectionParameters(user='invalid', password='invalid', database='invalid', ssl=<ssl.SSLContext object at 0x000002302... 'postgres'>, server_settings=None, target_session_attrs=<SessionAttribute.any: 'any'>, krbsrvname=None, gsslib='sspi')

    async def __connect_addr(
        params,
        retry,
        addr,
        loop,
        config,
        connection_class,
        record_class,
        params_input,
    ):
        connected = _create_future(loop)
    
        proto_factory = lambda: protocol.Protocol(
            addr, connected, params, record_class, loop)
    
        if isinstance(addr, str):
            # UNIX socket
            connector = loop.create_unix_connection(proto_factory, addr)
    
        elif params.ssl and params.ssl_negotiation is SSLNegotiation.direct:
            # if ssl and ssl_negotiation is `direct`, skip STARTTLS and perform
            # direct SSL connection
            connector = loop.create_connection(
                proto_factory, *addr, ssl=params.ssl
            )
    
        elif params.ssl:
            connector = _create_ssl_connection(
                proto_factory, *addr, loop=loop, ssl_context=params.ssl,
                ssl_is_advisory=params.sslmode == SSLMode.prefer)
        else:
            connector = loop.create_connection(proto_factory, *addr)
    
>       tr, pr = await connector
                 ^^^^^^^^^^^^^^^

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\asyncpg\connect_utils.py:1099: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

protocol_factory = <function __connect_addr.<locals>.<lambda> at 0x0000023028123240>
host = 'localhost', port = 5432

    async def _create_ssl_connection(
        # TODO: The return type is a specific combination of subclasses of
        # asyncio.protocols.Protocol that we can't express. For now, having the
        # return type be dependent on signature of the factory is an improvement
        protocol_factory: Callable[[], _ProctolFactoryR],
        host: str,
        port: int,
        *,
        loop: asyncio.AbstractEventLoop,
        ssl_context: ssl_module.SSLContext,
        ssl_is_advisory: bool = False,
    ) -> typing.Tuple[asyncio.Transport, _ProctolFactoryR]:
    
>       tr, pr = await loop.create_connection(
            lambda: TLSUpgradeProto(loop, host, port,
                                    ssl_context, ssl_is_advisory),
            host, port)

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\asyncpg\connect_utils.py:969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ProactorEventLoop running=False closed=False debug=False>
protocol_factory = <function _create_ssl_connection.<locals>.<lambda> at 0x00000230281232E0>
host = 'localhost', port = 5432

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            ssl_shutdown_timeout=None,
            happy_eyeballs_delay=None, interleave=None,
            all_errors=False):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if ssl_shutdown_timeout is not None and not ssl:
            raise ValueError(
                'ssl_shutdown_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock = (await staggered.staggered_race(
                    (
                        # can't use functools.partial as it keeps a reference
                        # to exceptions
                        lambda addrinfo=addrinfo: self._connect_sock(
                            exceptions, addrinfo, laddr_infos
                        )
                        for addrinfo in infos
                    ),
                    happy_eyeballs_delay,
                    loop=self,
                ))[0]  # can't use sock, _, _ as it keeks a reference to exceptions
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                try:
                    if all_errors:
                        raise ExceptionGroup("create_connection failed", exceptions)
                    if len(exceptions) == 1:
                        raise exceptions[0]
                    elif exceptions:
                        # If they all have the same str(), raise one.
                        model = str(exceptions[0])
                        if all(str(exc) == model for exc in exceptions):
>                           raise exceptions[0]

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py:1176: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ProactorEventLoop running=False closed=False debug=False>
protocol_factory = <function _create_ssl_connection.<locals>.<lambda> at 0x00000230281232E0>
host = 'localhost', port = 5432

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            ssl_shutdown_timeout=None,
            happy_eyeballs_delay=None, interleave=None,
            all_errors=False):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if ssl_shutdown_timeout is not None and not ssl:
            raise ValueError(
                'ssl_shutdown_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
>                       sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py:1146: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ProactorEventLoop running=False closed=False debug=False>
exceptions = None
addr_info = (<AddressFamily.AF_INET6: 23>, <SocketKind.SOCK_STREAM: 1>, 0, '', ('::1', 5432, 0, 0))
local_addr_infos = None

    async def _connect_sock(self, exceptions, addr_info, local_addr_infos=None):
        """Create, bind and connect one socket."""
        my_exceptions = []
        exceptions.append(my_exceptions)
        family, type_, proto, _, address = addr_info
        sock = None
        try:
            try:
                sock = socket.socket(family=family, type=type_, proto=proto)
                sock.setblocking(False)
                if local_addr_infos is not None:
                    for lfamily, _, _, _, laddr in local_addr_infos:
                        # skip local addresses of different family
                        if lfamily != family:
                            continue
                        try:
                            sock.bind(laddr)
                            break
                        except OSError as exc:
                            msg = (
                                f'error while attempting to bind on '
                                f'address {laddr!r}: {str(exc).lower()}'
                            )
                            exc = OSError(exc.errno, msg)
                            my_exceptions.append(exc)
                    else:  # all bind attempts failed
                        if my_exceptions:
                            raise my_exceptions.pop()
                        else:
                            raise OSError(f"no matching local address with {family=} found")
>               await self.sock_connect(sock, address)

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py:1045: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ProactorEventLoop running=False closed=False debug=False>
sock = <socket.socket [closed] fd=-1, family=23, type=1, proto=0>
address = ('::1', 5432, 0, 0)

    async def sock_connect(self, sock, address):
        if self._debug and sock.gettimeout() != 0:
            raise ValueError("the socket must be non-blocking")
>       return await self._proactor.connect(sock, address)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\Lib\asyncio\proactor_events.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <IocpProactor overlapped#=1 result#=0>, timeout = 59.989557900000364

    def _poll(self, timeout=None):
        if timeout is None:
            ms = INFINITE
        elif timeout < 0:
            raise ValueError("negative timeout")
        else:
            # GetQueuedCompletionStatus() has a resolution of 1 millisecond,
            # round away from zero to wait *at least* timeout seconds.
            ms = math.ceil(timeout * 1e3)
            if ms >= INFINITE:
                raise ValueError("timeout too big")
    
        while True:
            status = _overlapped.GetQueuedCompletionStatus(self._iocp, ms)
            if status is None:
                break
            ms = 0
    
            err, transferred, key, address = status
            try:
                f, ov, obj, callback = self._cache.pop(address)
            except KeyError:
                if self._loop.get_debug():
                    self._loop.call_exception_handler({
                        'message': ('GetQueuedCompletionStatus() returned an '
                                    'unexpected event'),
                        'status': ('err=%s transferred=%s key=%#x address=%#x'
                                   % (err, transferred, key, address)),
                    })
    
                # key is either zero, or it is used to return a pipe
                # handle which should be closed to avoid a leak.
                if key not in (0, _overlapped.INVALID_HANDLE_VALUE):
                    _winapi.CloseHandle(key)
                continue
    
            if obj in self._stopped_serving:
                f.cancel()
            # Don't call the callback if _register() already read the result or
            # if the overlapped has been cancelled
            elif not f.done():
                try:
>                   value = callback(transferred, key, ov)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\Lib\asyncio\windows_events.py:804: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

trans = 0, key = 0, ov = <_overlapped.Overlapped object at 0x0000023028081170>

    def finish_connect(trans, key, ov):
>       ov.getresult()
E       ConnectionRefusedError: [WinError 1225] The remote computer refused the network connection

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\Lib\asyncio\windows_events.py:600: ConnectionRefusedError

During handling of the above exception, another exception occurred:

    @pytest.mark.asyncio
    async def test_db_connection_fail_graceful():
        """Test that DB manager handles connection failure gracefully."""
        # Use invalid URL
        invalid_manager = DatabaseManager(url="postgresql+asyncpg://invalid:invalid@localhost:5432/invalid")
>       await invalid_manager.connect()

tests\test_database_simple.py:11: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <market_liquidity_monitor.data_engine.database.DatabaseManager object at 0x0000023022FE4B90>

    async def connect(self):
        """Test connection and create tables."""
        try:
            async with self.engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
            self._is_active = True
            print("\u2705 Connected to PostgreSQL database")
        except Exception as e:
>           print(f"\u26a0\ufe0f PostgreSQL connection failed: {e}")

data_engine\database.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <encodings.cp1252.IncrementalEncoder object at 0x000002301E034AD0>
input = '\u26a0\ufe0f PostgreSQL connection failed: [WinError 1225] The remote computer refused the network connection'
final = False

    def encode(self, input, final=False):
>       return codecs.charmap_encode(input,self.errors,encoding_table)[0]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       UnicodeEncodeError: 'charmap' codec can't encode characters in position 0-1: character maps to <undefined>

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\Lib\encodings\cp1252.py:19: UnicodeEncodeError
__________________________ test_store_snapshot_mock ___________________________

mock_create_engine = <MagicMock name='create_async_engine' id='2405853869936'>
mock_sessionmaker = <MagicMock name='async_sessionmaker' id='2405861002272'>

    @pytest.mark.asyncio
    @patch('market_liquidity_monitor.data_engine.database.async_sessionmaker')
    @patch('market_liquidity_monitor.data_engine.database.create_async_engine')
    async def test_store_snapshot_mock(mock_create_engine, mock_sessionmaker):
        """Test storing snapshot with mocked DB."""
        class MockSession:
            def __init__(self):
                self.add = MagicMock()
            async def __aenter__(self): return self
            async def __aexit__(self, *args): pass
            def begin(self):
                class MockBegin:
                    async def __aenter__(self): return None
                    async def __aexit__(self, *args): pass
                return MockBegin()
    
        manager = DatabaseManager()
        manager._is_active = True
    
        mock_session = MockSession()
        factory_instance = MagicMock()
        mock_sessionmaker.return_value = factory_instance
        factory_instance.return_value = mock_session
    
        snapshot_data = {"symbol": "BTC/USDT", "exchange": "binance", "best_bid": 100.0}
>       await manager.store_snapshot(snapshot_data)

tests\test_database_simple.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <market_liquidity_monitor.data_engine.database.DatabaseManager object at 0x00000230280AF390>
snapshot_data = {'best_bid': 100.0, 'exchange': 'binance', 'symbol': 'BTC/USDT'}

    async def store_snapshot(self, snapshot_data: Dict[str, Any]):
        """Store a new snapshot in the database."""
        if not self._is_active:
            return
    
        print(f"DEBUG: session_factory type: {type(self.session_factory)}")
        async with self.session_factory() as session:
            print(f"DEBUG: session type: {type(session)}")
            print(f"DEBUG: session.begin type: {type(session.begin)}")
            begin_obj = session.begin()
            print(f"DEBUG: begin_obj type: {type(begin_obj)}")
>           async with begin_obj:
                       ^^^^^^^^^
E           TypeError: 'coroutine' object does not support the asynchronous context manager protocol

data_engine\database.py:89: TypeError
============================== warnings summary ===============================
data_engine\models.py:199
  C:\Users\click\OneDrive\Documents\mlm\market_liquidity_monitor\data_engine\models.py:199: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class HistoricalSnapshot(BaseModel):

..\..\..\..\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\pydantic\_internal\_generate_schema.py:319
  C:\Users\click\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\pydantic\_internal\_generate_schema.py:319: PydanticDeprecatedSince20: `json_encoders` is deprecated. See https://docs.pydantic.dev/2.12/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED tests/test_database_simple.py::test_db_connection_fail_graceful - Unic...
FAILED tests/test_database_simple.py::test_store_snapshot_mock - TypeError: '...
=================== 2 failed, 1 passed, 2 warnings in 8.63s ===================
